{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "ref: https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- homogeneous ensemble model: use a single base learning algorithm , homogeneous weak learners are trained in different ways\n",
    "\n",
    "\n",
    "- heterogeneous ensemble model: use different type of base learning algorithms, some heterogeneous weak learners are combined\n",
    "\n",
    "\n",
    "- coherent with the way we aggregate base models: if we choose base models with low bias but high variance, it should be with an aggregating method that tends to reduce variance, vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bagging: consider as homogeneous weak learners, learns independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "\n",
    "\n",
    "- boosting: consider as homogeneous weak learners, learns base models sequentially in a very adaptive way(a base model depends on the previous one) and combines them following a deterministic strategy\n",
    "\n",
    "\n",
    "- stacking: consider as heterogeneous weak learners, leans in parallel and combines them by training a meta-model to output, a prediction based on the different weak models predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "stand for bootstrap aggregating \n",
    "\n",
    "help to reduce variance, prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate samples of size B from an initial dataset of size N by randomly drawing **with replacement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resampled data contains different characteristics that are as a whole in the original data\n",
    "\n",
    "\n",
    "It draws the distribution present in the data points, and also tend to remain different from each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avoid the problem of overfitting by using different sets of training data. The model becomes resilient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create bootstrapped samples\n",
    "\n",
    "2. apply a regression or classification algorithm to each sample\n",
    "\n",
    "3. for regression, take the average over all the outputs predicted by the individual learners\n",
    "\n",
    "    for classification, either the most voted class is accepted (hard-voting), or the highest average of all the class probabilities is taken as the output (soft-voting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- works well when the learners are unstable and tend to overfit\n",
    "\n",
    "    i.e. small changes in the training data lead to major changes in the predicted output. \n",
    "\n",
    "\n",
    "- reduces the variance by aggregating the individual learners composed of different statistical properties,\n",
    "\n",
    "    such as different standard deviations, means, etc. \n",
    "\n",
    "\n",
    "- works well for high variance models such as Decision Trees. The number of base learners (trees) to be chosen depends on the characteristics of the dataset. Using too many trees doesn’t lead to overfitting, but can consume a lot of computational power.\n",
    "\n",
    "\n",
    "- performs well on high-dimensional data\n",
    "\n",
    "\n",
    "- the missing values in the dataset do not affect the performance of the algorithm\n",
    "\n",
    "\n",
    "\n",
    "- when used with low variance models such as linear regression, it doesn’t really affect the learning process. \n",
    "\n",
    "\n",
    "- final prediction is based on the mean predictions from the subset trees, rather than outputting the precise values for the classification or regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API-BaggingRegressor:https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n",
    "\n",
    "API-BaggingClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "\n",
    "- base_estimator: The algorithm to be used on all the random subsets of the dataset. Default value is a decision tree.\n",
    "\n",
    "\n",
    "- n_estimators: The number of base estimators in the ensemble. Default value is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.861 (0.042)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "n_redundant=5, random_state=5)\n",
    "\n",
    "# define the model\n",
    "model = BaggingClassifier()\n",
    "\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1,error_score='raise')\n",
    "\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
